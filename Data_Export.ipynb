{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH8AieuU8riW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sZgdJTd9gom"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataExport.csv\").dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLPNU_fo2YZs"
   },
   "outputs": [],
   "source": [
    "path = 'D:\\Learning\\DataExport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSJ-wXLmS8QP"
   },
   "outputs": [],
   "source": [
    "corpus = '. '.join(df.iloc[:,2:].melt()['value'].values).lower()\n",
    "open(path + 'real_estates.txt', 'w', encoding='utf-8').write(corpus)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZ0fh3KSkM84"
   },
   "outputs": [],
   "source": [
    "with open(path + '/replace_with_2_spaces.txt', 'r', encoding='utf-8') as f:\n",
    "    text_need_replace = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAtTr3B-XQsH"
   },
   "outputs": [],
   "source": [
    "#removing emojis\n",
    "import re\n",
    "def remove_emojis(string):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\" \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', string)\n",
    "\n",
    "def remove_link(string):\n",
    "  return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "\n",
    "def remove(string):\n",
    "  lis = ['*', '=', ',', '..', '-', '_', '/', '+', '^', '?', '<', '>', ';', '{', '}', '[', ']', '!', '|', '&', '$', '~', '`', ')', '(']\n",
    "  for i in lis:\n",
    "    string = string.replace(i, '')\n",
    "  return string\n",
    "\n",
    "def remove_space(string):\n",
    "  return \" \".join(string.split())\n",
    "\n",
    "def replace_with_2_space(text):\n",
    "    abbr_to_replace = [txt.split(' ', 1) for txt in text_need_replace.split('\\n')]\n",
    "    for line in abbr_to_replace:\n",
    "        old = line[0]\n",
    "        new = line[1]\n",
    "        text = text.replace(old, new)\n",
    "        text = text.replace(new, new)\n",
    "    return text\n",
    "\n",
    "def solve(string):\n",
    "  func = [remove_emojis, remove_link, remove, remove_space, replace_with_2_space]\n",
    "  for i in func:\n",
    "    string = i(string)\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23X52TXcZXIc"
   },
   "outputs": [],
   "source": [
    "corpus = solve(corpus)\n",
    "open(path+'real_estates_after.txt', 'w', encoding='utf-8').write(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYoEhy96K8l9"
   },
   "outputs": [],
   "source": [
    "pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXltNUXhJM5z"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install underthesea nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UVJ6YsLGLld"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import re\n",
    "from underthesea import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, Laplace, KneserNeyInterpolated, WittenBellInterpolated\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SF8dVnFIJJGD"
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(path + 'real_estates_after.txt', 'r', encoding='utf-8') as f:\n",
    "    real_estates_corpus = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFXndKidZdih"
   },
   "source": [
    "Tiếp theo là các bước tiền xử lí cơ bản để lấy đầu vào cho mô hình n-gram:\n",
    "\n",
    "- Tách đoạn văn ra thành từng câu nhỏ\n",
    "- Loại bỏ các dấu câu trong câu\n",
    "- Tách từ (tokenize) với mỗi câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6FSvk1YTPfM"
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens]\n",
    "    tokens = [word for word in tokens if word] # remove empty\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRYLg0CLk9X1"
   },
   "outputs": [],
   "source": [
    "sent_corpus = sent_tokenize(corpus) # tách câu\n",
    "len(sent_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep2CTu1JZPdg"
   },
   "outputs": [],
   "source": [
    "word_corpus = [tokenize(sent) for sent in sent_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBuf1SnjZIZH"
   },
   "outputs": [],
   "source": [
    "n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0hjWwd6WHy_"
   },
   "outputs": [],
   "source": [
    "# tách theo n gram\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, word_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evEFR8jjWI4p"
   },
   "outputs": [],
   "source": [
    "vi_model = KneserNeyInterpolated(n) # tạo mô hình n-gram interpolation\n",
    "vi_model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmbve1lCWKwO"
   },
   "outputs": [],
   "source": [
    "def generate_sent(model, max_num_words, pre_words=[]):\n",
    "    content = pre_words\n",
    "    for i in range(max_num_words):\n",
    "        token = model.generate(1, text_seed=content[-(n - 1):])\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA5IR7RPYC3X"
   },
   "outputs": [],
   "source": [
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_86Pu0icZyDi"
   },
   "outputs": [],
   "source": [
    "def recommend_real_estates_search(txt_input):\n",
    "#     txt_input = pipeline_preprocessing_corpus(txt_input)\n",
    "    tokens = word_tokenize(txt_input)\n",
    "    output_recommend = []\n",
    "    for i in range(1, 10):\n",
    "        output = generate_sent(vi_model, i, tokens.copy())\n",
    "        output_recommend.append(detokenize(output))\n",
    "    output_recommend = list(set(output_recommend)) \n",
    "    return '\\n'.join(output_recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE0-FOi0Z5U8"
   },
   "outputs": [],
   "source": [
    "txt_input = input()\n",
    "print(recommend_real_estates_search(txt_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE63jYfEs4oT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3KSNYZHrp+At8KCmQ/idF",
   "name": "Data_Export.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
